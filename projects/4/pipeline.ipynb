{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcadea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.connect import *\n",
    "spark = Start().config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f06d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88eb8a6",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce2ca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "\n",
    "df = spark.read.json(\"/datasets/amazon/train.json\")\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84154466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "stages = []\n",
    "\n",
    "categoricalColumns = [\"reviewerName\", \"summary\", \"reviewText\"]\n",
    "\n",
    "tokenizer = Tokenizer(inputCol= \"reviewText\", outputCol=\"tokens\")\n",
    "hashingTF = HashingTF(inputCol = 'tokens', outputCol = 'rawFeatures')\n",
    "idf = IDF(inputCol = 'rawFeatures', outputCol = 'TfIdfFeatures', minDocFreq = 5)\n",
    "word2Vec = Word2Vec(inputCol = 'tokens', outputCol = 'Word2VecFeatures')\n",
    "countVec = CountVectorizer(inputCol = 'tokens', \n",
    "                           outputCol = 'CountVectFeatures')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "categoricalColumns = [\"reviewerName\", \"summary\", \"reviewText\"]\n",
    "for col in categoricalColumns:\n",
    "    tokenizer = Tokenizer(inputCol= col, outputCol=f\"{col}tokens\")\n",
    "    df = tokenizer.transform(df)\n",
    "    hasher = HashingTF(numFeatures=100, binary=True, inputCol=col, outputCol=f\"{col}_vector\")\n",
    "    df = hasher.transform(df)\n",
    "    df = df.drop(col, f\"{col}tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = VectorAssembler(inputCols=['asin',\n",
    "                                 'unixReviewTime',\n",
    "                                 'verified',\n",
    "                                 'vote',\n",
    "                                 'reviewerNametokens',\n",
    "                                 'summarytokens',\n",
    "                                 'reviewTexttokens'],\n",
    "                      outputCol='FEATURES')\n",
    "\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=12345)\n",
    "vector_feature_train = vec.transform(train)\n",
    "vector_feature_test = vec.transform(test)\n",
    "train = vector_feature_train.select('overall', 'FEATURES')\n",
    "test = vector_feature_test.select('overall', 'FEATURES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fed3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol='FEATURES', labelCol='overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b42cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction',\n",
    "                                labelCol='overall',\n",
    "                                metricName = \"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8866e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbt_model = gbt.fit(train)\n",
    "pred = gbt_model.transform(test)\n",
    "rmse = evaluator.evaluate(pred)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86bc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [10, 20, 30])\\\n",
    "    .addGrid(gbt.maxDepth, [3, 4, 5])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22408e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train)\n",
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e294730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"overall\", predictionCol=\"pred\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfc1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "dsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
